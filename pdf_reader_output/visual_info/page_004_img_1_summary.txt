Summary for page_004_img_1.jpg
====================
The image is a page from a document that discusses Scaled Dot-Product Attention
(SDPA) and Multi-Head Attention (MHA). These attention mechanisms are used in
neural networks, particularly in natural language processing tasks.  SDPA is an
attention mechanism where the query vector's dot product with all key vectors is
computed, and then scaled down by dividing by the square root of the dimension
of the keys vector (dk) and applying a softmax function to obtain weights on the
value vector (V). The final output is obtained by multiplying the weighted value
vector with the transpose of the key matrix.  MHA is an extension of SDPA where
the queries, keys, and values are linearly projected h times with different
learned linear projections to dk, dk, and dv dimensions, respectively. The
attention function is then performed on each projected version in parallel,
yielding a dv-dimensional output vector.  The document mentions that dot-product
attention is identical to SDPA, except for the scaling factor of 1√dk. Additive
attention, on the other hand, computes the compatibility function using a feed-
forward network with a single hidden layer. The two attention mechanisms have
similar theoretical complexity, but dot-product attention is faster and more
space-efficient in practice due to its ability to be implemented using highly
optimized matrix multiplication code.  The document also notes that for small
values of dk, the two mechanisms perform similarly, but additive attention
outperforms dot product attention without scaling for larger values of dk. This
is because the dot products grow large in magnitude, causing the softmax
function to have extremely small gradients, which can be counteracted by scaling
the dot products by 1√dk.  The image also contains a caption that states "Scaled
Dot-Product Attention" on the left and "Multi-Head Attention" on the right. The
left side of the image shows a graphical representation of SDPA, while the right
side shows a similar representation for MHA, where the attention layers are
running in parallel.