Summary for page_004_img_2.jpg
====================
The image presents an explanation of Scaled Dot-Product Attention and Multi-Head
Attention from a document titled "Multi-Head Attention". These attention
mechanisms are used in natural language processing (NLP) for various tasks such
as machine translation, text summarization, and language generation.  Scaled
Dot-Product Attention is the focus of the first section of the image. This
attention mechanism takes queries, keys, and values as input, computes the dot
product of each query with all keys, divides each by √dk, applies a softmax
function to obtain weights on the values, and finally multiplies the weights
with the values to produce the output. The two most commonly used attention
functions are additive attention and dot-product attention. Additive attention
is slower and less space-efficient than dot-product attention in practice, but
for small values of dk, they perform similarly. However, as the value of dk
increases, dot-product attention outperforms additive attention without scaling.
To counteract this effect, dot-product attention scales the dot products by
1√dk.  The second section of the image discusses Multi-Head Attention. Instead
of performing a single attention function with dmodel-dimensional keys, values,
and queries, Multi-Head Attention linearly projects the queries, keys, and
values h times with different, learned linear projections to dk, dk, and dv
dimensions, respectively. On each projected version of queries, keys, and
values, the attention function is then performed in parallel, yielding dv-
dimensional outputs.  The summary of this graph or chart image is that Scaled
Dot-Product Attention and Multi-Head Attention are important attention
mechanisms used in NLP tasks. These mechanisms differ in their performance for
small and large values of dk, and in the case of Multi-Head Attention, the
number of linear projections performed in parallel.