Summary for page_004_img_2.jpg
====================
The image provides an overview of Scaled Dot-Product Attention and Multi-Head
Attention in the context of machine learning, specifically in the field of
natural language processing (NLP). These attention mechanisms are used to focus
on specific parts of a sequence when processing text data.  Scaled Dot-Product
Attention is shown on the left side of Figure 2. It works by computing the dot
product of queries with keys and then dividing each result by the square root of
the key dimension (dk). The resulting values are then normalized using a softmax
function to obtain weights on the values. In practice, this operation is
performed on sets of queries, keys, and values packed together into matrices Q,
K, and V.  Multi-Head Attention, depicted on the right side of Figure 2,
consists of several attention layers running in parallel. This means that
instead of performing a single attention function with dmodel-dimensional keys,
values, and queries, each set of queries, keys, and values is linearly projected
h times with different learned linear projections to dk, dk, and dv dimensions,
respectively. The attention function is then performed in parallel on these
projected versions, yielding dv-dimensional outputs.  The text accompanying the
image explains that dot-product attention is identical to Scaled Dot-Product
Attention, except for the scaling factor of 1√dk. Additive attention, on the
other hand, computes the compatibility function using a feed-forward network
with a single hidden layer. While both mechanisms have similar theoretical
complexity, dot-product attention is much faster and more space-efficient in
practice due to its ability to be implemented using highly optimized matrix
multiplication code.  The text also highlights that for small values of dk, the
two attention mechanisms perform similarly. However, additive attention
outperforms dot-product attention without scaling for larger values of dk
because the dot products grow large in magnitude, pushing the softmax function
into regions where it has extremely small gradients. To counteract this effect,
dot-product attention scales the dot products by 1√dk.  In summary, the image
and text provide an explanation of two key attention mechanisms in NLP - Scaled
Dot-Product Attention and Multi-Head Attention. These mechanisms are essential
components of modern NLP models, allowing them to focus on relevant parts of a
sequence when processing language data.